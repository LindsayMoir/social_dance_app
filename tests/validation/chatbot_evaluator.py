"""
Chatbot Batch Testing Module

Generates test questions, executes them through the chatbot, and scores results.

This module contains four main components:
1. TestQuestionGenerator - Generates questions from templates
2. ChatbotTestExecutor - Executes questions through chatbot pipeline
3. ChatbotScorer - Scores results using LLM evaluation
4. generate_chatbot_report() - Creates comprehensive reports

Author: Claude Code
Version: 1.0.0
"""

from datetime import datetime
from itertools import product
import json
import logging
import numpy as np
import os
import pandas as pd
import re
import time
from typing import Dict, List, Optional
from zoneinfo import ZoneInfo
import yaml


class TestQuestionGenerator:
    """
    Generates test questions from YAML templates.

    Supports two question types:
    - Template-based: Questions generated by combining parameters
    - Static: Pre-written natural language questions

    Attributes:
        template_file (str): Path to YAML template file
        templates (dict): Loaded template configuration
    """

    def __init__(self, template_file: str = 'tests/data/chatbot_test_questions.yaml'):
        """
        Initialize the TestQuestionGenerator.

        Args:
            template_file (str): Path to YAML file with question templates
        """
        self.template_file = template_file

        # Load templates
        if not os.path.exists(template_file):
            raise FileNotFoundError(f"Template file not found: {template_file}")

        with open(template_file, 'r') as f:
            self.templates = yaml.safe_load(f)

        logging.info(f"Loaded question templates from: {template_file}")

    def generate_all_questions(self) -> List[dict]:
        """
        Generate all test questions from templates.

        Returns:
            List[dict]: List of question dictionaries with keys:
                - question (str): The question text
                - category (str): Question category
                - parameters (dict): Template parameters (for template-based)
                - expected_criteria (dict): Expected criteria for scoring
        """
        questions = []

        for template_config in self.templates['templates']:
            category = template_config['category']

            if 'static_questions' in template_config:
                # Static questions (natural language)
                for question_text in template_config['static_questions']:
                    questions.append({
                        'question': question_text,
                        'category': category,
                        'expected_criteria': self._extract_criteria(question_text)
                    })
            else:
                # Template-based generation
                questions.extend(self._generate_from_template(template_config))

        logging.info(f"Generated {len(questions)} test questions across {len(self.templates['templates'])} categories")
        return questions

    def _generate_from_template(self, config: dict) -> List[dict]:
        """
        Generate questions by combining template parameters.

        Args:
            config (dict): Template configuration with template string and parameters

        Returns:
            List[dict]: Generated question dictionaries
        """
        template = config['template']
        param_names = list(config['parameters'].keys())
        param_values = [config['parameters'][name] for name in param_names]

        questions = []

        # Generate all combinations using itertools.product
        for combo in product(*param_values):
            params = dict(zip(param_names, combo))
            question_text = template.format(**params)

            questions.append({
                'question': question_text,
                'category': config['category'],
                'parameters': params,
                'expected_criteria': params  # Parameters ARE the expected criteria
            })

        return questions

    def _extract_criteria(self, question: str) -> dict:
        """
        Extract expected criteria from natural language question.

        Uses keyword matching to identify dance styles, event types, and timeframes.

        Args:
            question (str): Natural language question

        Returns:
            dict: Extracted criteria (dance_style, event_type, timeframe)
        """
        criteria = {}

        # Dance styles (extended list)
        styles = [
            'salsa', 'bachata', 'tango', 'west coast swing', 'wcs',
            'kizomba', 'zouk', 'lindy hop', 'balboa', 'swing'
        ]
        for style in styles:
            if style.lower() in question.lower():
                criteria['dance_style'] = style
                break

        # Event types
        if any(word in question.lower() for word in ['class', 'learn', 'beginner']):
            criteria['event_type'] = 'class'
        elif 'social' in question.lower():
            criteria['event_type'] = 'social dance'
        elif 'workshop' in question.lower():
            criteria['event_type'] = 'workshop'

        # Timeframes
        if 'tonight' in question.lower():
            criteria['timeframe'] = 'tonight'
        elif 'weekend' in question.lower():
            criteria['timeframe'] = 'this weekend'
        elif 'week' in question.lower():
            criteria['timeframe'] = 'this week'

        return criteria


class ChatbotTestExecutor:
    """
    Executes test questions through the chatbot pipeline.

    Uses the actual chatbot prompt and SQL generation logic to ensure
    tests reflect real production behavior.

    Attributes:
        config (dict): Configuration dictionary
        db_handler: DatabaseHandler instance
        llm_handler: LLMHandler instance
        sql_prompt_template (str): Loaded SQL generation prompt
    """

    def __init__(self, config: dict, db_handler):
        """
        Initialize the ChatbotTestExecutor.

        Args:
            config (dict): Configuration dictionary
            db_handler: DatabaseHandler instance
        """
        self.config = config
        self.db_handler = db_handler

        # Import LLMHandler
        import sys
        sys.path.insert(0, 'src')
        from llm import LLMHandler

        self.llm_handler = LLMHandler(config_path='config/config.yaml')

        # Load actual chatbot SQL prompt
        prompt_path = 'prompts/contextual_sql_prompt.txt'
        if not os.path.exists(prompt_path):
            raise FileNotFoundError(f"SQL prompt file not found: {prompt_path}")

        with open(prompt_path, 'r') as f:
            self.sql_prompt_template = f.read()

        logging.info("ChatbotTestExecutor initialized with production prompt")

    def execute_test_question(self, question_dict: dict) -> dict:
        """
        Execute a single test question through full chatbot pipeline.

        Steps:
        1. Format prompt with current date/time context
        2. Query LLM for SQL generation
        3. Sanitize SQL (same logic as main.py)
        4. Execute SQL and capture results

        Args:
            question_dict (dict): Question dictionary from TestQuestionGenerator

        Returns:
            dict: Test result with SQL, results, execution status
        """
        question = question_dict['question']

        # Get current date context (Pacific timezone - same as chatbot)
        pacific_tz = ZoneInfo("America/Los_Angeles")
        now = datetime.now(pacific_tz)
        current_date = now.strftime("%Y-%m-%d")
        current_day_of_week = now.strftime("%A")

        # Build prompt (simplified - no conversation history for batch tests)
        prompt = self.sql_prompt_template.format(
            context_info="",
            conversation_history="",
            intent="search",
            entities=str(question_dict.get('parameters', {})),
            current_date=current_date,
            current_day_of_week=current_day_of_week
        )
        prompt += f"\n\nCurrent User Question: \"{question}\""

        try:
            # Import date calculator tool
            from date_calculator import CALCULATE_DATE_RANGE_TOOL

            # Query LLM for SQL with date calculator tool support
            sql_raw = self.llm_handler.query_llm('', prompt, tools=[CALCULATE_DATE_RANGE_TOOL])

            if not sql_raw:
                return self._create_error_result(question_dict, "LLM returned empty response")

            # Check for CLARIFICATION response (part of normal confirmation workflow)
            if sql_raw.strip().startswith("CLARIFICATION:"):
                # Extract just the clarification text (after "CLARIFICATION:")
                clarification_text = sql_raw.strip().replace("CLARIFICATION:", "").strip()

                return {
                    'question': question,
                    'category': question_dict['category'],
                    'expected_criteria': question_dict.get('expected_criteria', {}),
                    'sql_query': sql_raw.strip(),
                    'clarification_text': clarification_text,  # Store for scorer evaluation
                    'sql_syntax_valid': True,  # CLARIFICATION is valid output
                    'execution_success': True,  # Successfully returned clarification
                    'result_count': 0,  # No SQL results expected
                    'sample_results': [],
                    'timestamp': datetime.now().isoformat(),
                    'is_clarification': True  # Flag for scorer
                }

            # Sanitize SQL (same logic as main.py lines 369-375)
            sql_query = sql_raw.replace("```sql", "").replace("```", "").strip()
            select_idx = sql_query.upper().find("SELECT")
            if select_idx != -1:
                sql_query = sql_query[select_idx:]
            sql_query = sql_query.split(";")[0].strip()

            # Validate SQL syntax
            syntax_valid = self._check_sql_syntax(sql_query)

            # Execute query if valid
            if syntax_valid:
                try:
                    results = self.db_handler.execute_query(sql_query)
                    execution_success = True
                    result_count = len(results) if results else 0

                    # Get sample results (first 5)
                    if results:
                        # Convert to list of dicts for easier handling
                        sample_results = [dict(row._mapping) for row in results[:5]]
                    else:
                        sample_results = []

                except Exception as e:
                    logging.error(f"SQL execution failed for question '{question}': {e}")
                    execution_success = False
                    result_count = 0
                    sample_results = []
                    sql_query = sql_query  # Keep SQL for debugging

            else:
                execution_success = False
                result_count = 0
                sample_results = []

            return {
                'question': question,
                'category': question_dict['category'],
                'expected_criteria': question_dict.get('expected_criteria', {}),
                'sql_query': sql_query,
                'sql_syntax_valid': syntax_valid,
                'execution_success': execution_success,
                'result_count': result_count,
                'sample_results': sample_results,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logging.error(f"Test execution failed for question '{question}': {e}")
            return self._create_error_result(question_dict, str(e))

    def _check_sql_syntax(self, sql_query: str) -> bool:
        """
        Basic SQL validation.

        Checks for:
        - SELECT keyword presence
        - FROM events clause
        - Basic SQL structure

        Args:
            sql_query (str): SQL query string

        Returns:
            bool: True if syntax appears valid
        """
        if not sql_query:
            return False

        sql_upper = sql_query.upper()

        # Must contain SELECT
        if 'SELECT' not in sql_upper:
            return False

        # Must query events table (handle multiline SQL with extra whitespace)
        import re
        if not re.search(r'FROM\s+EVENTS', sql_upper):
            return False

        # Try to parse with sqlalchemy (basic validation)
        try:
            from sqlalchemy import text
            text(sql_query)
            return True
        except Exception as e:
            logging.warning(f"SQL parsing failed: {e}")
            return False

    def _create_error_result(self, question_dict: dict, error_message: str) -> dict:
        """Helper to create error result dictionary."""
        return {
            'question': question_dict['question'],
            'category': question_dict['category'],
            'expected_criteria': question_dict.get('expected_criteria', {}),
            'sql_query': None,
            'sql_syntax_valid': False,
            'execution_success': False,
            'result_count': 0,
            'sample_results': [],
            'error': error_message,
            'timestamp': datetime.now().isoformat()
        }

    def execute_all_tests(self, questions: List[dict]) -> List[dict]:
        """
        Execute all test questions.

        Args:
            questions (List[dict]): List of question dictionaries

        Returns:
            List[dict]: List of test results
        """
        results = []
        total = len(questions)

        logging.info(f"Executing {total} test questions...")

        for i, question_dict in enumerate(questions, 1):
            logging.info(f"Executing test {i}/{total}: {question_dict['question'][:60]}...")

            result = self.execute_test_question(question_dict)
            results.append(result)

            # Small delay to avoid API rate limits
            time.sleep(0.5)

        logging.info(f"Completed {len(results)} test executions")
        return results


class ChatbotScorer:
    """
    Scores chatbot test results using LLM evaluation.

    Uses LLM to evaluate if SQL results match user intent, with
    fallback to rule-based scoring if LLM fails.

    Attributes:
        llm_handler: LLMHandler instance
        eval_prompt (str): LLM evaluation prompt template
    """

    def __init__(self, llm_handler):
        """
        Initialize the ChatbotScorer.

        Args:
            llm_handler: LLMHandler instance
        """
        self.llm_handler = llm_handler

        # LLM evaluation prompt
        self.eval_prompt = """You are evaluating SQL query results for a dance events chatbot.

IMPORTANT CONTEXT: Today's date is {current_date}. The current year is {current_year}.

USER QUESTION: {question}

EXPECTED CRITERIA: {expected_criteria}

GENERATED SQL: {sql_query}

SAMPLE RESULTS (first 5 events): {sample_results}

TOTAL RESULT COUNT: {result_count}

Evaluate whether the SQL query correctly addresses the user's intent. Consider:
1. Does the SQL filter by the correct criteria (dance style, timeframe, venue, event type)?
2. Are date/time filters correct for the requested timeframe?
3. Is the result count reasonable (0 results may indicate incorrect query)?
4. Do the sample results match what the user asked for?

Respond with ONLY valid JSON:
{{
  "score": <integer 0-100>,
  "reasoning": "<brief 1-2 sentence explanation>",
  "criteria_matched": ["<criterion1>", "<criterion2>"],
  "criteria_missed": ["<criterion1>"],
  "sql_issues": ["<issue1>", "<issue2>"]
}}

Score guidelines:
- 90-100: Perfect query, results exactly match intent
- 70-89: Good query, minor issues (e.g., date range slightly off)
- 50-69: Partially correct, some criteria missing
- 30-49: Major issues, wrong filters or logic
- 0-29: Query failed or completely wrong
"""

    def score_result(self, test_result: dict) -> dict:
        """
        Score a single test result using LLM evaluation.

        Args:
            test_result (dict): Test result from ChatbotTestExecutor

        Returns:
            dict: Evaluation with score, reasoning, criteria analysis
        """
        # Handle CLARIFICATION responses
        # NOTE: CLARIFICATIONS are part of the normal workflow - the chatbot returns a
        # confirmation/interpretation for EVERY query before executing SQL.
        # We score based on whether the clarification text is accurate, not whether
        # a clarification was returned.
        if test_result.get('is_clarification', False):
            question = test_result['question'].lower()
            clarification_text = test_result.get('clarification_text', '').lower()

            # For weekend queries, check if clarification includes Friday
            if 'weekend' in question:
                # Weekend should include Friday, Saturday, Sunday
                has_friday = 'friday' in clarification_text
                has_saturday = 'saturday' in clarification_text
                has_sunday = 'sunday' in clarification_text

                if has_friday and has_saturday and has_sunday:
                    return {
                        'score': 95,
                        'reasoning': 'CLARIFICATION correctly identifies weekend as Friday, Saturday, and Sunday',
                        'criteria_matched': ['weekend_definition', 'confirmation_workflow'],
                        'criteria_missed': [],
                        'sql_issues': []
                    }
                elif has_saturday and has_sunday and not has_friday:
                    return {
                        'score': 40,
                        'reasoning': 'CLARIFICATION incorrectly defines weekend as only Saturday and Sunday (should include Friday night)',
                        'criteria_matched': ['confirmation_workflow'],
                        'criteria_missed': ['weekend_definition'],
                        'sql_issues': ['Missing Friday in weekend definition']
                    }

            # For other queries, CLARIFICATION is part of normal workflow
            return {
                'score': 90,
                'reasoning': 'CLARIFICATION returned as part of normal confirmation workflow',
                'criteria_matched': ['confirmation_workflow'],
                'criteria_missed': [],
                'sql_issues': []
            }

        # Auto-fail if query didn't execute
        if not test_result['execution_success']:
            return {
                'score': 0,
                'reasoning': f"Query execution failed: {test_result.get('error', 'Unknown error')}",
                'criteria_matched': [],
                'criteria_missed': list(test_result['expected_criteria'].keys()),
                'sql_issues': ['execution_failure']
            }

        # Format evaluation prompt with current date context
        current_date = datetime.now().strftime('%Y-%m-%d')
        current_year = datetime.now().year

        prompt = self.eval_prompt.format(
            current_date=current_date,
            current_year=current_year,
            question=test_result['question'],
            expected_criteria=json.dumps(test_result['expected_criteria'], indent=2),
            sql_query=test_result['sql_query'],
            sample_results=json.dumps(test_result['sample_results'], indent=2, default=str)[:1000],  # Limit size
            result_count=test_result['result_count']
        )

        try:
            # Query LLM for evaluation
            response = self.llm_handler.query_llm('', prompt)

            if not response:
                logging.warning("LLM evaluation returned empty response, using fallback")
                return self._fallback_scoring(test_result)

            # Parse JSON response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                eval_result = json.loads(json_match.group())

                # Validate score is in range
                if 'score' in eval_result:
                    eval_result['score'] = max(0, min(100, eval_result['score']))

                return eval_result
            else:
                logging.warning("No JSON found in LLM response, using fallback")
                return self._fallback_scoring(test_result)

        except Exception as e:
            logging.error(f"LLM scoring failed: {e}")
            return self._fallback_scoring(test_result)

    def _fallback_scoring(self, test_result: dict) -> dict:
        """
        Simple rule-based scoring if LLM fails.

        Checks if expected criteria keywords appear in SQL.

        Args:
            test_result (dict): Test result dictionary

        Returns:
            dict: Fallback evaluation
        """
        score = 50  # Base score for successful execution
        criteria_matched = []
        criteria_missed = []

        sql = test_result['sql_query'].lower() if test_result['sql_query'] else ""
        expected = test_result['expected_criteria']

        # Check dance style
        if 'dance_style' in expected:
            if expected['dance_style'].lower() in sql:
                score += 20
                criteria_matched.append('dance_style')
            else:
                criteria_missed.append('dance_style')

        # Check timeframe (look for date-related SQL)
        if 'timeframe' in expected:
            time_keywords = ['current_date', 'interval', 'extract(dow', 'start_date']
            if any(kw in sql for kw in time_keywords):
                score += 15
                criteria_matched.append('timeframe')
            else:
                criteria_missed.append('timeframe')

        # Check venue
        if 'venue' in expected:
            if expected['venue'].lower() in sql:
                score += 15
                criteria_matched.append('venue')
            else:
                criteria_missed.append('venue')

        # Check event type
        if 'event_type' in expected:
            if expected['event_type'].lower() in sql:
                score += 10
                criteria_matched.append('event_type')
            else:
                criteria_missed.append('event_type')

        return {
            'score': min(score, 100),
            'reasoning': 'Fallback rule-based scoring (LLM evaluation failed)',
            'criteria_matched': criteria_matched,
            'criteria_missed': criteria_missed,
            'sql_issues': []
        }

    def score_all_results(self, test_results: List[dict]) -> List[dict]:
        """
        Score all test results.

        Args:
            test_results (List[dict]): List of test results

        Returns:
            List[dict]: Test results with added 'evaluation' field
        """
        logging.info(f"Scoring {len(test_results)} test results...")

        for i, result in enumerate(test_results, 1):
            logging.info(f"Scoring result {i}/{len(test_results)}...")
            result['evaluation'] = self.score_result(result)

            # Small delay to avoid API rate limits
            time.sleep(0.3)

        logging.info("Scoring complete")
        return test_results


def generate_chatbot_report(scored_results: List[dict], output_dir: str = 'tests/output') -> dict:
    """
    Generate comprehensive chatbot testing report.

    Creates:
    - CSV with all results
    - JSON with summary + analysis

    Args:
        scored_results (List[dict]): Scored test results
        output_dir (str): Output directory path

    Returns:
        dict: Report dictionary
    """
    os.makedirs(output_dir, exist_ok=True)

    # Save detailed results to CSV
    try:
        # Flatten evaluation dict for CSV
        flattened_results = []
        for result in scored_results:
            flat = result.copy()
            if 'evaluation' in flat:
                eval_data = flat.pop('evaluation')
                flat['evaluation_score'] = eval_data.get('score', 0)
                flat['evaluation_reasoning'] = eval_data.get('reasoning', '')
                flat['criteria_matched'] = ', '.join(eval_data.get('criteria_matched', []))
                flat['criteria_missed'] = ', '.join(eval_data.get('criteria_missed', []))
                flat['sql_issues'] = ', '.join(eval_data.get('sql_issues', []))

            # Remove complex fields for CSV
            flat.pop('sample_results', None)
            flat.pop('expected_criteria', None)
            flat.pop('parameters', None)

            flattened_results.append(flat)

        results_df = pd.DataFrame(flattened_results)
        csv_path = os.path.join(output_dir, 'chatbot_test_results.csv')
        results_df.to_csv(csv_path, index=False)
        logging.info(f"CSV results saved: {csv_path}")

    except Exception as e:
        logging.error(f"Failed to save CSV: {e}")

    # Calculate summary statistics
    total_tests = len(scored_results)
    execution_success_count = sum(1 for r in scored_results if r['execution_success'])
    execution_success_rate = execution_success_count / total_tests if total_tests > 0 else 0

    scores = [r['evaluation']['score'] for r in scored_results]
    avg_score = np.mean(scores) if scores else 0

    # Score distribution
    score_buckets = {
        'excellent (90-100)': sum(1 for s in scores if s >= 90),
        'good (70-89)': sum(1 for s in scores if 70 <= s < 90),
        'fair (50-69)': sum(1 for s in scores if 50 <= s < 70),
        'poor (<50)': sum(1 for s in scores if s < 50)
    }

    # Category breakdown
    categories = set(r['category'] for r in scored_results)
    category_stats = {}

    for category in categories:
        cat_results = [r for r in scored_results if r['category'] == category]
        cat_scores = [r['evaluation']['score'] for r in cat_results]
        cat_success = sum(1 for r in cat_results if r['execution_success'])

        category_stats[category] = {
            'count': len(cat_results),
            'avg_score': float(np.mean(cat_scores)) if cat_scores else 0,
            'success_rate': cat_success / len(cat_results) if cat_results else 0
        }

    # Identify problematic questions (score < 50)
    problematic = [
        {
            'question': r['question'],
            'category': r['category'],
            'score': r['evaluation']['score'],
            'reasoning': r['evaluation']['reasoning'],
            'sql_query': r.get('sql_query', 'N/A')
        }
        for r in scored_results
        if r['evaluation']['score'] < 50
    ]

    # Build JSON report
    report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_tests': total_tests,
            'execution_success_rate': float(execution_success_rate),
            'average_score': float(avg_score),
            'score_distribution': score_buckets
        },
        'category_breakdown': category_stats,
        'problematic_questions': problematic[:20],  # Limit to first 20
        'recommendations': _generate_recommendations(scored_results, category_stats)
    }

    # Save JSON report
    try:
        json_path = os.path.join(output_dir, 'chatbot_evaluation_report.json')
        with open(json_path, 'w') as f:
            json.dump(report, f, indent=2)
        logging.info(f"JSON report saved: {json_path}")
    except Exception as e:
        logging.error(f"Failed to save JSON report: {e}")

    return report


def _generate_recommendations(results: List[dict], category_stats: dict) -> List[dict]:
    """
    Generate actionable recommendations based on test results.

    Args:
        results (List[dict]): Scored test results
        category_stats (dict): Category breakdown statistics

    Returns:
        List[dict]: Recommendations
    """
    recommendations = []

    # Check for low-performing categories
    for category, stats in category_stats.items():
        if stats['avg_score'] < 60:
            recommendations.append({
                'issue': f'Low performance in category: {category}',
                'avg_score': stats['avg_score'],
                'recommendation': f'Review prompt handling for {category} queries. Consider adding examples in contextual_sql_prompt.txt'
            })

    # Check for common SQL issues
    sql_issues = []
    for r in results:
        sql_issues.extend(r['evaluation'].get('sql_issues', []))

    if sql_issues:
        issue_counts = pd.Series(sql_issues).value_counts()
        for issue, count in issue_counts.items():
            if count > 5:  # Frequent issue
                recommendations.append({
                    'issue': f'Frequent SQL issue: {issue}',
                    'frequency': int(count),
                    'recommendation': f'Review SQL generation logic for {issue}'
                })

    # Check execution success rate
    exec_rate = sum(1 for r in results if r['execution_success']) / len(results)
    if exec_rate < 0.90:
        recommendations.append({
            'issue': 'Low SQL execution success rate',
            'execution_rate': float(exec_rate),
            'recommendation': 'Review SQL syntax validation and error handling'
        })

    return recommendations
