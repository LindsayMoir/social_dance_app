"""
Chatbot Batch Testing Module

Generates test questions, executes them through the chatbot, and scores results.

This module contains four main components:
1. TestQuestionGenerator - Generates questions from templates
2. ChatbotTestExecutor - Executes questions through chatbot pipeline
3. ChatbotScorer - Scores results using LLM evaluation
4. generate_chatbot_report() - Creates comprehensive reports

Author: Claude Code
Version: 1.0.0
"""

from datetime import datetime
from itertools import product
import json
import logging
import numpy as np
import os
import pandas as pd
import re
import time
from typing import Dict, List, Optional
from zoneinfo import ZoneInfo
import yaml


class TestQuestionGenerator:
    """
    Generates test questions from YAML templates.

    Supports two question types:
    - Template-based: Questions generated by combining parameters
    - Static: Pre-written natural language questions

    Attributes:
        template_file (str): Path to YAML template file
        templates (dict): Loaded template configuration
    """

    def __init__(self, template_file: str = 'tests/data/chatbot_test_questions.yaml'):
        """
        Initialize the TestQuestionGenerator.

        Args:
            template_file (str): Path to YAML file with question templates
        """
        self.template_file = template_file

        # Load templates
        if not os.path.exists(template_file):
            raise FileNotFoundError(f"Template file not found: {template_file}")

        with open(template_file, 'r') as f:
            self.templates = yaml.safe_load(f)

        logging.info(f"Loaded question templates from: {template_file}")

    def generate_all_questions(self) -> List[dict]:
        """
        Generate all test questions from templates.

        Returns:
            List[dict]: List of question dictionaries with keys:
                - question (str): The question text
                - category (str): Question category
                - parameters (dict): Template parameters (for template-based)
                - expected_criteria (dict): Expected criteria for scoring
        """
        questions = []

        for template_config in self.templates['templates']:
            category = template_config['category']

            if 'static_questions' in template_config:
                # Static questions (natural language)
                for question_text in template_config['static_questions']:
                    questions.append({
                        'question': question_text,
                        'category': category,
                        'expected_criteria': self._extract_criteria(question_text)
                    })
            else:
                # Template-based generation
                questions.extend(self._generate_from_template(template_config))

        logging.info(f"Generated {len(questions)} test questions across {len(self.templates['templates'])} categories")
        return questions

    def _generate_from_template(self, config: dict) -> List[dict]:
        """
        Generate questions by combining template parameters.

        Args:
            config (dict): Template configuration with template string and parameters

        Returns:
            List[dict]: Generated question dictionaries
        """
        template = config['template']
        param_names = list(config['parameters'].keys())
        param_values = [config['parameters'][name] for name in param_names]

        questions = []

        # Generate all combinations using itertools.product
        for combo in product(*param_values):
            params = dict(zip(param_names, combo))
            question_text = template.format(**params)

            questions.append({
                'question': question_text,
                'category': config['category'],
                'parameters': params,
                'expected_criteria': params  # Parameters ARE the expected criteria
            })

        return questions

    def _extract_criteria(self, question: str) -> dict:
        """
        Extract expected criteria from natural language question.

        Uses keyword matching to identify dance styles, event types, and timeframes.

        Args:
            question (str): Natural language question

        Returns:
            dict: Extracted criteria (dance_style, event_type, timeframe)
        """
        criteria = {}

        # Dance styles (extended list)
        styles = [
            'salsa', 'bachata', 'tango', 'west coast swing', 'wcs',
            'kizomba', 'zouk', 'lindy hop', 'balboa', 'swing'
        ]
        for style in styles:
            if style.lower() in question.lower():
                criteria['dance_style'] = style
                break

        # Event types
        if any(word in question.lower() for word in ['class', 'learn', 'beginner']):
            criteria['event_type'] = 'class'
        elif 'social' in question.lower():
            criteria['event_type'] = 'social dance'
        elif 'workshop' in question.lower():
            criteria['event_type'] = 'workshop'

        # Timeframes
        if 'tonight' in question.lower():
            criteria['timeframe'] = 'tonight'
        elif 'weekend' in question.lower():
            criteria['timeframe'] = 'this weekend'
        elif 'week' in question.lower():
            criteria['timeframe'] = 'this week'

        return criteria


class ChatbotTestExecutor:
    """
    Executes test questions through the chatbot pipeline.

    Uses the actual chatbot prompt and SQL generation logic to ensure
    tests reflect real production behavior.

    Attributes:
        config (dict): Configuration dictionary
        db_handler: DatabaseHandler instance
        llm_handler: LLMHandler instance
        sql_prompt_template (str): Loaded SQL generation prompt
    """

    def __init__(self, config: dict, db_handler):
        """
        Initialize the ChatbotTestExecutor.

        Args:
            config (dict): Configuration dictionary
            db_handler: DatabaseHandler instance
        """
        self.config = config
        self.db_handler = db_handler

        # Import LLMHandler
        import sys
        sys.path.insert(0, 'src')
        from llm import LLMHandler

        self.llm_handler = LLMHandler(config_path='config/config.yaml')

        # Load actual chatbot SQL prompt (path from config if provided)
        prompt_path = (
            self.config.get('prompts', {})
                .get('contextual_sql', {})
                .get('file', 'prompts/contextual_sql_prompt.txt')
        )
        if not os.path.exists(prompt_path):
            raise FileNotFoundError(f"SQL prompt file not found: {prompt_path}")

        with open(prompt_path, 'r') as f:
            self.sql_prompt_template = f.read()

        # Load interpretation prompt (for confirmation text generation)
        interpretation_prompt_path = (
            self.config.get('prompts', {})
                .get('interpretation', {})
                .get('file', 'prompts/interpretation_prompt.txt')
        )
        if not os.path.exists(interpretation_prompt_path):
            raise FileNotFoundError(f"Interpretation prompt file not found: {interpretation_prompt_path}")

        with open(interpretation_prompt_path, 'r') as f:
            self.interpretation_prompt_template = f.read()

        logging.info("ChatbotTestExecutor initialized with production prompts")

    @staticmethod
    def _sql_has_illegal_date_arithmetic(sql: str) -> bool:
        if not sql:
            return False
        s = sql.upper()
        patterns = [
            r"\bCURRENT_DATE\s*[\+\-]\s*\d+\b",
            r"\bCURRENT_TIMESTAMP\s*[\+\-]\s*\d+\b",
            r"'\d{4}-\d{2}-\d{2}'\s*[\+\-]",
        ]
        return any(re.search(p, s) for p in patterns)

    def generate_interpretation(self, user_query: str, current_date: str, current_day_of_week: str, current_time: str) -> str:
        """
        Generate a natural language interpretation of the user's search intent.
        This mirrors the generate_interpretation() function in main.py.

        Args:
            user_query: The user's input query
            current_date: Current date in YYYY-MM-DD format
            current_day_of_week: Current day name (e.g., "Monday")
            current_time: Current time with timezone (e.g., "14:30 PST")

        Returns:
            str: Natural language interpretation of the search intent
        """
        default_city = self.config.get('location', {}).get('epicentre', 'Victoria, British Columbia, Canada')

        # Format the interpretation prompt
        formatted_prompt = self.interpretation_prompt_template.format(
            current_date=current_date,
            current_day_of_week=current_day_of_week,
            current_time=current_time,
            default_city=default_city,
            user_query=user_query
        )

        try:
            # Query LLM for interpretation WITH date calculator tool
            from date_calculator import CALCULATE_DATE_RANGE_TOOL
            interpretation = self.llm_handler.query_llm('', formatted_prompt, tools=[CALCULATE_DATE_RANGE_TOOL])

            if not interpretation:
                return f"My understanding is that you want to search for dance events related to: {user_query}"

            return interpretation.strip()

        except Exception as e:
            logging.error(f"Error generating interpretation: {e}")
            return f"My understanding is that you want to search for: {user_query}"

    def execute_test_question(self, question_dict: dict) -> dict:
        """
        Execute a single test question through full chatbot pipeline.

        Steps:
        1. Format prompt with current date/time context
        2. Query LLM for SQL generation
        3. Sanitize SQL (same logic as main.py)
        4. Execute SQL and capture results

        Args:
            question_dict (dict): Question dictionary from TestQuestionGenerator

        Returns:
            dict: Test result with SQL, results, execution status
        """
        question = question_dict['question']

        # Get current date context (Pacific timezone - same as chatbot)
        pacific_tz = ZoneInfo("America/Los_Angeles")
        now = datetime.now(pacific_tz)
        current_date = now.strftime("%Y-%m-%d")
        current_day_of_week = now.strftime("%A")
        current_time = now.strftime("%H:%M %Z")

        # STEP 1: Generate interpretation (confirmation text)
        interpretation = self.generate_interpretation(
            user_query=question,
            current_date=current_date,
            current_day_of_week=current_day_of_week,
            current_time=current_time
        )

        # Build prompt (simplified - no conversation history for batch tests)
        prompt = self.sql_prompt_template.format(
            context_info="",
            conversation_history="",
            intent="search",
            entities=str(question_dict.get('parameters', {})),
            current_date=current_date,
            current_day_of_week=current_day_of_week
        )
        prompt += f"\n\nCurrent User Question: \"{question}\""

        try:
            # Import date calculator tool
            from date_calculator import CALCULATE_DATE_RANGE_TOOL

            # Query LLM for SQL with date calculator tool support
            sql_raw = self.llm_handler.query_llm('', prompt, tools=[CALCULATE_DATE_RANGE_TOOL])

            if not sql_raw:
                return self._create_error_result(question_dict, "LLM returned empty response")

            # CLARIFICATION path
            if sql_raw.strip().startswith("CLARIFICATION:"):
                clarification_text = sql_raw.strip().replace("CLARIFICATION:", "").strip()
                logging.info("QUESTION: %s", question)
                logging.info("CLARIFICATION TEXT: %s", clarification_text)
                return {
                    'question': question,
                    'category': question_dict['category'],
                    'expected_criteria': question_dict.get('expected_criteria', {}),
                    'interpretation': interpretation,
                    'sql_query': sql_raw.strip(),
                    'clarification_text': clarification_text,
                    'sql_syntax_valid': True,
                    'execution_success': True,
                    'result_count': 0,
                    'sample_results': [],
                    'timestamp': datetime.now().isoformat(),
                    'current_date_used': current_date,
                    'current_timezone': "America/Los_Angeles",
                    'is_clarification': True
                }

            # Sanitize SQL
            sql_query = sql_raw.replace("```sql", "").replace("```", "").strip()
            select_idx = sql_query.upper().find("SELECT")
            if select_idx != -1:
                sql_query = sql_query[select_idx:]
            sql_query = sql_query.split(";")[0].strip()

            # Preflight re-query if illegal date arithmetic is detected
            if self._sql_has_illegal_date_arithmetic(sql_query):
                strict_suffix = (
                    "\n\nSTRICT FIX: You MUST call calculate_date_range for ANY temporal expression and use ONLY the returned dates. "
                    "Never add/subtract integers to dates (e.g., CURRENT_DATE + 7). If referencing CURRENT_DATE, use INTERVAL syntax only, "
                    "but prefer explicit dates from the tool. Return ONLY SQL."
                )
                strict_prompt = f"{prompt}\n{strict_suffix}"
                sql_raw2 = self.llm_handler.query_llm('', strict_prompt, tools=[CALCULATE_DATE_RANGE_TOOL])
                if sql_raw2:
                    sql2 = sql_raw2.replace("```sql", "").replace("```", "").strip()
                    si2 = sql2.upper().find("SELECT")
                    if si2 != -1:
                        sql2 = sql2[si2:]
                    sql2 = sql2.split(";")[0].strip()
                    if not self._sql_has_illegal_date_arithmetic(sql2):
                        sql_query = sql2

            # Log the final SQL that will be validated/executed
            logging.info("QUESTION: %s", question)
            logging.info("SQL:\n%s", sql_query or "<none>")

            # Validate SQL syntax
            syntax_valid = self._check_sql_syntax(sql_query)
            if not syntax_valid and self._sql_has_illegal_date_arithmetic(sql_query):
                logging.warning("Preflight: Invalid SQL with illegal date arithmetic detected. Attempting re-query.")

            # Execute query if valid
            if syntax_valid:
                try:
                    results = self.db_handler.execute_query(sql_query)
                    execution_success = True
                    result_count = len(results) if results else 0

                    # Get sample results (first 5)
                    if results:
                        sample_results = [dict(row._mapping) for row in results[:5]]
                    else:
                        sample_results = []

                except Exception as e:
                    logging.error(f"SQL execution failed for question '{question}': {e}")
                    execution_success = False
                    result_count = 0
                    sample_results = []
            else:
                execution_success = False
                result_count = 0
                sample_results = []

            return {
                'question': question,
                'category': question_dict['category'],
                'expected_criteria': question_dict.get('expected_criteria', {}),
                'interpretation': interpretation,
                'sql_query': sql_query,
                'sql_syntax_valid': syntax_valid,
                'execution_success': execution_success,
                'result_count': result_count,
                'sample_results': sample_results,
                'timestamp': datetime.now().isoformat(),
                'current_date_used': current_date,
                'current_timezone': "America/Los_Angeles"
            }

        except Exception as e:
            logging.error(f"Test execution failed for question '{question}': {e}")
            return self._create_error_result(question_dict, str(e))

    def _check_sql_syntax(self, sql_query: str) -> bool:
        """
        Basic SQL validation.

        Checks for:
        - SELECT keyword presence
        - FROM events clause
        - Basic SQL structure

        Args:
            sql_query (str): SQL query string

        Returns:
            bool: True if syntax appears valid
        """
        if not sql_query:
            return False

        sql_upper = sql_query.upper()

        # Must contain SELECT
        if 'SELECT' not in sql_upper:
            return False

        # Must query events table (handle multiline SQL with extra whitespace)
        import re
        if not re.search(r'FROM\s+EVENTS', sql_upper):
            return False

        # Try to parse with sqlalchemy (basic validation)
        try:
            from sqlalchemy import text
            text(sql_query)
            return True
        except Exception as e:
            logging.warning(f"SQL parsing failed: {e}")
            return False

    def _create_error_result(self, question_dict: dict, error_message: str) -> dict:
        """Helper to create error result dictionary."""
        return {
            'question': question_dict['question'],
            'category': question_dict['category'],
            'expected_criteria': question_dict.get('expected_criteria', {}),
            'sql_query': None,
            'sql_syntax_valid': False,
            'execution_success': False,
            'result_count': 0,
            'sample_results': [],
            'error': error_message,
            'timestamp': datetime.now().isoformat()
        }

    def execute_all_tests(self, questions: List[dict]) -> List[dict]:
        """
        Execute all test questions.

        Args:
            questions (List[dict]): List of question dictionaries

        Returns:
            List[dict]: List of test results
        """
        results = []
        total = len(questions)

        logging.info(f"Executing {total} test questions...")

        for i, question_dict in enumerate(questions, 1):
            logging.info(f"Executing test {i}/{total}: {question_dict['question'][:60]}...")

            result = self.execute_test_question(question_dict)
            results.append(result)

            # Small delay to avoid API rate limits
            time.sleep(0.5)

        logging.info(f"Completed {len(results)} test executions")
        return results


class ChatbotScorer:
    """
    Scores chatbot test results using LLM evaluation.

    Uses LLM to evaluate if SQL results match user intent, with
    fallback to rule-based scoring if LLM fails.

    Attributes:
        llm_handler: LLMHandler instance
        eval_prompt (str): LLM evaluation prompt template
    """

    def __init__(self, llm_handler):
        """
        Initialize the ChatbotScorer.

        Args:
            llm_handler: LLMHandler instance
        """
        self.llm_handler = llm_handler

        # LLM evaluation prompt
        self.eval_prompt = """You are evaluating SQL query results for a dance events chatbot.

IMPORTANT CONTEXT: Today's date is {current_date}. The current year is {current_year}.

USER QUESTION: {question}

EXPECTED CRITERIA: {expected_criteria}

GENERATED SQL: {sql_query}

SAMPLE RESULTS (first 5 events): {sample_results}

TOTAL RESULT COUNT: {result_count}

PROJECT TEMPORAL POLICIES (STRICT):
- Weekend definition for dance events = Friday through Sunday (inclusive), no special time cutoff.
  • APPLY THIS ONLY when the user's timeframe explicitly mentions weekend ("this weekend", "next weekend", or an equivalent weekend phrasing).
- Week boundaries = Sunday–Saturday. "This week" means today through Saturday; "Next week" means Sunday–Saturday of the following week.
  • DO NOT apply weekend-only constraints to week-based queries. Do not penalize a 'this week' or 'next week' query for not enforcing a Friday cutoff.
- Timezone: use the local Pacific time context when interpreting dates/times.

EVALUATION CRITERIA:
1. Does the SQL filter by the correct criteria (dance style, timeframe, venue, event type)?
2. Are date/time filters correct for the requested timeframe (respect the Weekend/Week policies above, and only apply the weekend rule when the query asks for weekend)?
3. Is the result count reasonable (0 results may indicate incorrect query)?
4. Do the sample results match what the user asked for?

DEFAULT EVENT TYPE POLICY (CRITICAL):
- By default, queries should include social dances unless the user explicitly restricts event types (e.g., "only classes", "live music only").
- If the user later adds other types (classes, workshops, live music), they should be OR'ed in with social dance.
- Do NOT penalize queries for including event_type ILIKE '%social dance%' by default. That is correct and expected unless the user requested otherwise.

LEARNING-EVENT SYNONYMS (IMPORTANT):
- "Class" and "Workshop" are considered learning-oriented and interchangeable for most intents.
- If a user asks for classes, including workshops (event_type ILIKE '%workshop%') alongside classes is acceptable and should NOT be penalized.
- Only penalize inclusion of workshops when the user explicitly restricts to classes only (e.g., "classes only", "no workshops").

EVENT-TYPE CONJUNCTION POLICY (AND vs OR):
- Natural language like "classes and social dances", "workshops and social dancing", or "classes with social" should be interpreted as OR across event types (either type qualifies) unless the user explicitly requires both in the same event.
- Acceptable patterns include (event_type ILIKE '%class%' OR event_type ILIKE '%workshop%' OR event_type ILIKE '%social dance%').
- Do NOT penalize the use of OR for these phrasings. Only require AND (same-event conjunction) when the user is explicit (e.g., "class followed by social in one event", "combined class+social").

TONIGHT / NIGHTTIME POLICY (CRITICAL):
- For phrases like "tonight" or "tomorrow night", it is CORRECT to filter using start_date = <that day> AND start_time >= '18:00:00'.
- Do NOT penalize queries for not including events that started earlier but continue into the night, unless the user explicitly asks to include overlapping/ongoing events.
- "Tonight" does not require span/overlap logic by default; the 18:00 cutoff is the established policy.

Respond with ONLY valid JSON:
{{
  "score": <integer 0-100>,
  "reasoning": "<brief 1-2 sentence explanation>",
  "criteria_matched": ["<criterion1>", "<criterion2>"],
  "criteria_missed": ["<criterion1>"],
  "sql_issues": ["<issue1>", "<issue2>"]
}}

Score guidelines:
- 90-100: Perfect query, results exactly match intent and temporal policies
- 70-89: Good query, minor issues (e.g., date window slightly off but still mostly correct)
- 50-69: Partially correct, some criteria missing or temporal policy partially applied
- 30-49: Major issues, wrong filters or logic
- 0-29: Query failed or completely wrong
"""

    def _evaluate_interpretation(self, test_result: dict) -> dict:
        """
        Evaluate the interpretation text (confirmation message) for accuracy.

        Checks:
        - Weekend queries should mention Friday, Saturday, and Sunday
        - Date mentions should be accurate
        - Key details (dance style, venue, event type) should be included

        Args:
            test_result (dict): Test result with 'interpretation' field

        Returns:
            dict: Evaluation with score, issues found
        """
        interpretation = test_result.get('interpretation', '').lower()
        question = test_result['question'].lower()
        issues = []
        score = 100  # Start with perfect score

        # Check weekend definition (CRITICAL issue from user feedback)
        if 'weekend' in question:
            has_friday = 'friday' in interpretation
            has_saturday = 'saturday' in interpretation
            has_sunday = 'sunday' in interpretation

            if not has_friday:
                issues.append('Missing Friday in weekend definition')
                score -= 50  # Major issue
            if not has_saturday or not has_sunday:
                issues.append('Missing Saturday/Sunday in weekend definition')
                score -= 30

            if has_friday and has_saturday and has_sunday:
                return {
                    'score': 100,
                    'issues': [],
                    'passed': True
                }

        # Check that dance style is mentioned if in question
        dance_styles = ['salsa', 'bachata', 'tango', 'swing', 'kizomba', 'zouk', 'lindy hop', 'balboa']
        for style in dance_styles:
            if style in question and style not in interpretation:
                issues.append(f'Missing dance style: {style}')
                score -= 10

        # Check that venue is mentioned if in question
        venues = ['loft', 'dance victoria', 'method studios', 'edelweiss']
        for venue in venues:
            if venue in question and venue not in interpretation:
                issues.append(f'Missing venue: {venue}')
                score -= 10

        # Check that event type is mentioned if in question
        event_types = ['class', 'workshop', 'social', 'live music', 'rehearsal']
        for event_type in event_types:
            if event_type in question and event_type not in interpretation:
                issues.append(f'Missing event type: {event_type}')
                score -= 10

        return {
            'score': max(0, score),  # Don't go below 0
            'issues': issues,
            'passed': score >= 70
        }

    def score_result(self, test_result: dict) -> dict:
        """
        Score a single test result using LLM evaluation.

        Evaluates TWO aspects:
        1. Interpretation text accuracy (confirmation message shown to user)
        2. SQL query correctness (executed after user confirms)

        Args:
            test_result (dict): Test result from ChatbotTestExecutor

        Returns:
            dict: Evaluation with score, reasoning, criteria analysis, interpretation_evaluation
        """
        # STEP 1: Evaluate interpretation text (confirmation message)
        interpretation_evaluation = self._evaluate_interpretation(test_result)

        # STEP 2: Evaluate SQL/CLARIFICATION
        # Handle CLARIFICATION responses
        # NOTE: CLARIFICATIONS are part of the normal workflow - the chatbot returns a
        # confirmation/interpretation for EVERY query before executing SQL.
        # We score based on whether the clarification text is accurate, not whether
        # a clarification was returned.
        if test_result.get('is_clarification', False):
            question = test_result['question'].lower()
            clarification_text = test_result.get('clarification_text', '').lower()

            # For weekend queries, check if clarification includes Friday
            if 'weekend' in question:
                # Weekend should include Friday, Saturday, Sunday
                has_friday = 'friday' in clarification_text
                has_saturday = 'saturday' in clarification_text
                has_sunday = 'sunday' in clarification_text

                if has_friday and has_saturday and has_sunday:
                    return {
                        'score': 95,
                        'reasoning': 'CLARIFICATION correctly identifies weekend as Friday, Saturday, and Sunday',
                        'criteria_matched': ['weekend_definition', 'confirmation_workflow'],
                        'criteria_missed': [],
                        'sql_issues': [],
                        'interpretation_evaluation': interpretation_evaluation
                    }
                elif has_saturday and has_sunday and not has_friday:
                    return {
                        'score': 40,
                        'reasoning': 'CLARIFICATION incorrectly defines weekend as only Saturday and Sunday (should include Friday night)',
                        'criteria_matched': ['confirmation_workflow'],
                        'criteria_missed': ['weekend_definition'],
                        'sql_issues': ['Missing Friday in weekend definition'],
                        'interpretation_evaluation': interpretation_evaluation
                    }

            # For other queries, CLARIFICATION is part of normal workflow
            return {
                'score': 90,
                'reasoning': 'CLARIFICATION returned as part of normal confirmation workflow',
                'criteria_matched': ['confirmation_workflow'],
                'criteria_missed': [],
                'sql_issues': [],
                'interpretation_evaluation': interpretation_evaluation
            }

        # Auto-fail if query didn't execute
        if not test_result['execution_success']:
            return {
                'score': 0,
                'reasoning': f"Query execution failed: {test_result.get('error', 'Unknown error')}",
                'criteria_matched': [],
                'criteria_missed': list(test_result['expected_criteria'].keys()),
                'sql_issues': ['execution_failure'],
                'interpretation_evaluation': interpretation_evaluation
            }

        # Format evaluation prompt with current date context
        # Use the same date context used to generate SQL (Pacific) if available
        current_date = test_result.get('current_date_used') or datetime.now().strftime('%Y-%m-%d')
        try:
            current_year = int(current_date[:4])
        except Exception:
            current_year = datetime.now().year

        prompt = self.eval_prompt.format(
            current_date=current_date,
            current_year=current_year,
            question=test_result['question'],
            expected_criteria=json.dumps(test_result['expected_criteria'], indent=2),
            sql_query=test_result['sql_query'],
            sample_results=json.dumps(test_result['sample_results'], indent=2, default=str)[:1000],  # Limit size
            result_count=test_result['result_count']
        )

        try:
            # Query LLM for evaluation
            response = self.llm_handler.query_llm('', prompt)

            if not response:
                logging.warning("LLM evaluation returned empty response, using fallback")
                return self._fallback_scoring(test_result)

            # Parse JSON response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                eval_result = json.loads(json_match.group())

                # Validate score is in range
                if 'score' in eval_result:
                    eval_result['score'] = max(0, min(100, eval_result['score']))

                # Add interpretation evaluation
                eval_result['interpretation_evaluation'] = interpretation_evaluation
                return eval_result
            else:
                logging.warning("No JSON found in LLM response, using fallback")
                return self._fallback_scoring(test_result, interpretation_evaluation)

        except Exception as e:
            logging.error(f"LLM scoring failed: {e}")
            return self._fallback_scoring(test_result, interpretation_evaluation)

    def _fallback_scoring(self, test_result: dict, interpretation_evaluation: dict) -> dict:
        """
        Simple rule-based scoring if LLM fails.

        Checks if expected criteria keywords appear in SQL.

        Args:
            test_result (dict): Test result dictionary
            interpretation_evaluation (dict): Interpretation evaluation result

        Returns:
            dict: Fallback evaluation
        """
        score = 50  # Base score for successful execution
        criteria_matched = []
        criteria_missed = []

        sql = test_result['sql_query'].lower() if test_result['sql_query'] else ""
        expected = test_result['expected_criteria']

        # Check dance style
        if 'dance_style' in expected:
            if expected['dance_style'].lower() in sql:
                score += 20
                criteria_matched.append('dance_style')
            else:
                criteria_missed.append('dance_style')

        # Check timeframe (look for date-related SQL)
        if 'timeframe' in expected:
            time_keywords = ['current_date', 'interval', 'extract(dow', 'start_date']
            if any(kw in sql for kw in time_keywords):
                score += 15
                criteria_matched.append('timeframe')
            else:
                criteria_missed.append('timeframe')

        # Check venue
        if 'venue' in expected:
            if expected['venue'].lower() in sql:
                score += 15
                criteria_matched.append('venue')
            else:
                criteria_missed.append('venue')

        # Check event type
        if 'event_type' in expected:
            if expected['event_type'].lower() in sql:
                score += 10
                criteria_matched.append('event_type')
            else:
                criteria_missed.append('event_type')

        return {
            'score': min(score, 100),
            'reasoning': 'Fallback rule-based scoring (LLM evaluation failed)',
            'criteria_matched': criteria_matched,
            'criteria_missed': criteria_missed,
            'sql_issues': [],
            'interpretation_evaluation': interpretation_evaluation
        }

    def score_all_results(self, test_results: List[dict]) -> List[dict]:
        """
        Score all test results.

        Args:
            test_results (List[dict]): List of test results

        Returns:
            List[dict]: Test results with added 'evaluation' field
        """
        logging.info(f"Scoring {len(test_results)} test results...")

        for i, result in enumerate(test_results, 1):
            logging.info(f"Scoring result {i}/{len(test_results)}...")
            result['evaluation'] = self.score_result(result)

            # Small delay to avoid API rate limits
            time.sleep(0.3)

        logging.info("Scoring complete")
        return test_results


def generate_chatbot_report(scored_results: List[dict], output_dir: str = 'output') -> dict:
    """
    Generate comprehensive chatbot testing report.

    Creates:
    - CSV with all results
    - JSON with summary + analysis

    Args:
        scored_results (List[dict]): Scored test results
        output_dir (str): Output directory path

    Returns:
        dict: Report dictionary
    """
    os.makedirs(output_dir, exist_ok=True)

    # Save detailed results to CSV
    try:
            # Flatten evaluation dict for CSV
        flattened_results = []
        for result in scored_results:
            flat = result.copy()
            if 'evaluation' in flat:
                eval_data = flat.pop('evaluation')
                flat['evaluation_score'] = eval_data.get('score', 0)
                flat['evaluation_reasoning'] = eval_data.get('reasoning', '')
                flat['criteria_matched'] = ', '.join(eval_data.get('criteria_matched', []))
                flat['criteria_missed'] = ', '.join(eval_data.get('criteria_missed', []))
                flat['sql_issues'] = ', '.join(eval_data.get('sql_issues', []))

                # Add interpretation evaluation to CSV
                if 'interpretation_evaluation' in eval_data:
                    interp_eval = eval_data['interpretation_evaluation']
                    flat['interpretation_score'] = interp_eval.get('score', 0)
                    flat['interpretation_issues'] = ', '.join(interp_eval.get('issues', []))
                    flat['interpretation_passed'] = interp_eval.get('passed', True)

            # Remove complex fields for CSV
            flat.pop('sample_results', None)
            flat.pop('expected_criteria', None)
            flat.pop('parameters', None)

            flattened_results.append(flat)

        results_df = pd.DataFrame(flattened_results)
        csv_path = os.path.join(output_dir, 'chatbot_test_results.csv')
        results_df.to_csv(csv_path, index=False)
        logging.info(f"CSV results saved: {csv_path}")

    except Exception as e:
        logging.error(f"Failed to save CSV: {e}")

    # Calculate summary statistics
    total_tests = len(scored_results)
    execution_success_count = sum(1 for r in scored_results if r['execution_success'])
    execution_success_rate = execution_success_count / total_tests if total_tests > 0 else 0

    scores = [r['evaluation']['score'] for r in scored_results]
    avg_score = np.mean(scores) if scores else 0

    # Score distribution
    score_buckets = {
        'excellent (90-100)': sum(1 for s in scores if s >= 90),
        'good (70-89)': sum(1 for s in scores if 70 <= s < 90),
        'fair (50-69)': sum(1 for s in scores if 50 <= s < 70),
        'poor (<50)': sum(1 for s in scores if s < 50)
    }

    # Interpretation evaluation statistics
    interpretation_scores = []
    interpretation_failed = []
    for r in scored_results:
        if 'interpretation_evaluation' in r.get('evaluation', {}):
            interp_eval = r['evaluation']['interpretation_evaluation']
            interpretation_scores.append(interp_eval.get('score', 100))
            if not interp_eval.get('passed', True):
                interpretation_failed.append({
                    'question': r['question'],
                    'interpretation': r.get('interpretation', ''),
                    'issues': interp_eval.get('issues', []),
                    'score': interp_eval.get('score', 0)
                })

    avg_interpretation_score = np.mean(interpretation_scores) if interpretation_scores else 100
    interpretation_pass_rate = 1 - (len(interpretation_failed) / total_tests) if total_tests > 0 else 1

    # Category breakdown
    categories = set(r['category'] for r in scored_results)
    category_stats = {}

    for category in categories:
        cat_results = [r for r in scored_results if r['category'] == category]
        cat_scores = [r['evaluation']['score'] for r in cat_results]
        cat_success = sum(1 for r in cat_results if r['execution_success'])

        category_stats[category] = {
            'count': len(cat_results),
            'avg_score': float(np.mean(cat_scores)) if cat_scores else 0,
            'success_rate': cat_success / len(cat_results) if cat_results else 0
        }

    # Identify problematic questions (score < 50)
    problematic = [
        {
            'question': r['question'],
            'category': r['category'],
            'score': r['evaluation']['score'],
            'reasoning': r['evaluation']['reasoning'],
            'sql_query': r.get('sql_query', 'N/A')
        }
        for r in scored_results
        if r['evaluation']['score'] < 50
    ]

    # Build JSON report
    # Build Problem Categories (evaluation_score < 90)
    def _categorize_issue(row: dict) -> str:
        q = row.get('question', '').lower()
        reason = row.get('evaluation', {}).get('reasoning', '').lower()
        sql = (row.get('sql_query') or '').lower()

        # Prefer weekend classification first to avoid "week" substrings in reasoning misplacing weekend queries
        if any(k in q or k in reason for k in ["weekend", " fri", " sat", " sun", "friday", "saturday", "sunday"]):
            return "Weekend Calculation"

        # Week classification: ensure we don't accidentally match weekend
        week_terms = ["this week", "next week", "calendar week", "monday", "sunday", "week calculation"]
        if any((t in q or t in reason) for t in week_terms):
            return "Week Calculation"

        if any(k in q for k in ["tonight", "tomorrow night"]) or ("tonight" in reason):
            return "Tonight / Time Filter"
        if "event_type" in reason or ("social dance" in q and "event_type" in sql and "social" not in sql):
            return "Event Type Defaults"
        if any(k in sql for k in ["current_date +", "current_date -"]) or "interval" in reason:
            return "Date Arithmetic"
        return "Other"

    issues_under_90 = [r for r in scored_results if r['evaluation']['score'] < 90]
    buckets = {}
    for r in issues_under_90:
        cat = _categorize_issue(r)
        buckets.setdefault(cat, []).append(r)

    # Take top 5 by frequency
    top_cats = sorted(buckets.items(), key=lambda x: len(x[1]), reverse=True)[:5]
    problem_categories = []
    for name, rows in top_cats:
        example = rows[0]
        problem_categories.append({
            'name': name,
            'count': len(rows),
            'example': {
                'question': example.get('question', ''),
                'reason': example['evaluation'].get('reasoning', '')[:200],
                'sql': example.get('sql_query', '')
            },
            'questions': [r.get('question', '') for r in rows[:10]],
            'recommendation': 'Align policy and date tool; verify prompt and SQL filters.'
        })

    report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_tests': total_tests,
            'execution_success_rate': float(execution_success_rate),
            'average_score': float(avg_score),
            'score_distribution': score_buckets,
            'interpretation_evaluation': {
                'average_interpretation_score': float(avg_interpretation_score),
                'interpretation_pass_rate': float(interpretation_pass_rate),
                'failed_interpretations_count': len(interpretation_failed)
            }
        },
        'category_breakdown': category_stats,
        'problematic_questions': problematic[:20],  # Limit to first 20
        'problem_categories': problem_categories,
        'failed_interpretations': interpretation_failed[:20],  # Limit to first 20
        'recommendations': _generate_recommendations(scored_results, category_stats, interpretation_failed)
    }

    # Save JSON report
    try:
        json_path = os.path.join(output_dir, 'chatbot_evaluation_report.json')
        with open(json_path, 'w') as f:
            json.dump(report, f, indent=2)
        logging.info(f"JSON report saved: {json_path}")
    except Exception as e:
        logging.error(f"Failed to save JSON report: {e}")

    return report


def _generate_recommendations(results: List[dict], category_stats: dict, interpretation_failed: List[dict]) -> List[dict]:
    """
    Generate actionable recommendations based on test results.

    Args:
        results (List[dict]): Scored test results
        category_stats (dict): Category breakdown statistics
        interpretation_failed (List[dict]): Failed interpretation evaluations

    Returns:
        List[dict]: Recommendations
    """
    recommendations = []

    # Check for interpretation issues
    if interpretation_failed:
        # Count issue types
        issue_types = {}
        for failed in interpretation_failed:
            for issue in failed.get('issues', []):
                issue_types[issue] = issue_types.get(issue, 0) + 1

        # Add recommendations for common interpretation issues
        for issue, count in issue_types.items():
            if count > 2:  # Frequent issue
                recommendations.append({
                    'issue': f'Interpretation issue: {issue}',
                    'occurrences': count,
                    'recommendation': 'Review interpretation_prompt.txt and ensure date calculator tool is being used correctly'
                })


    # Check for low-performing categories
    for category, stats in category_stats.items():
        if stats['avg_score'] < 60:
            recommendations.append({
                'issue': f'Low performance in category: {category}',
                'avg_score': stats['avg_score'],
                'recommendation': f'Review prompt handling for {category} queries. Consider adding examples in contextual_sql_prompt.txt'
            })

    # Check for common SQL issues
    sql_issues = []
    for r in results:
        sql_issues.extend(r['evaluation'].get('sql_issues', []))

    if sql_issues:
        issue_counts = pd.Series(sql_issues).value_counts()
        for issue, count in issue_counts.items():
            if count > 5:  # Frequent issue
                recommendations.append({
                    'issue': f'Frequent SQL issue: {issue}',
                    'frequency': int(count),
                    'recommendation': f'Review SQL generation logic for {issue}'
                })

    # Check execution success rate
    exec_rate = sum(1 for r in results if r['execution_success']) / len(results)
    if exec_rate < 0.90:
        recommendations.append({
            'issue': 'Low SQL execution success rate',
            'execution_rate': float(exec_rate),
            'recommendation': 'Review SQL syntax validation and error handling'
        })

    return recommendations
